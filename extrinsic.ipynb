{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1653a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import *\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import random\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b3aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [2, 5, 10]\n",
    "min_count = 10\n",
    "vector_sizes = [0, 50, 100, 500]\n",
    "languages = ['bxr', 'myv', 'kv']\n",
    "methods = ['cbow', 'sg', 'glove', 'pmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe139062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "scores_d = {}\n",
    "for language in languages:\n",
    "    scores_d[language]={}\n",
    "    for vector_size in vector_sizes:\n",
    "        for window in windows:            \n",
    "            pt = \"./embeddings/\"+language+\"/\"+str(vector_size)+\"/\"+str(window)+\"/\"\n",
    "            path = Path(pt)\n",
    "            wes = list(path.iterdir())\n",
    "            print(wes)\n",
    "            for we in wes:\n",
    "                l = we.stem\n",
    "                \n",
    "                if l not in scores_d[language]:\n",
    "                    scores_d[language][l] = {}\n",
    "                    \n",
    "                if vector_size not in scores_d[language][l]:\n",
    "                    scores_d[language][l][vector_size] = {}\n",
    "                    \n",
    "                print(str(we.absolute()))\n",
    "                pos_score = pos(language, str(we.absolute()), epochs=10, vector_load=True)\n",
    "                print(\"score:\", pos_score)\n",
    "                scores_d[language][l][vector_size][window] = pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41992513-640c-4201-b019-4fba20d64d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpmiWeighting\n",
      "& 0  &  0.254 & 0.258 & 0.265 \\\\\n",
      "PlmiWeighting\n",
      "& 0  &  0.258 & 0.269 & 0.249 \\\\\n",
      "pmi\n",
      "& 0  &  0.301 & 0.308 & 0.318 \\\\\n",
      "& 50  &  0.221 & 0.171 & 0.193 \\\\\n",
      "& 100  &  0.218 & 0.215 & 0.226 \\\\\n",
      "& 500  &  0.219 & 0.216 & 0.218 \\\\\n",
      "cbow\n",
      "& 0  &  0.185 & 0.178 & 0.185 \\\\\n",
      "& 50  &  0.171 & 0.173 & 0.17 \\\\\n",
      "& 100  &  0.185 & 0.178 & 0.185 \\\\\n",
      "& 500  &  0.199 & 0.196 & 0.206 \\\\\n",
      "PlogWeighting\n",
      "& 0  &  0.317 & 0.323 & 0.305 \\\\\n",
      "PpmiWeighting\n",
      "& 0  &  0.301 & 0.308 & 0.318 \\\\\n",
      "ft\n",
      "& 50  &  0.188 & 0.188 & 0.192 \\\\\n",
      "& 100  &  0.198 & 0.191 & 0.197 \\\\\n",
      "& 500  &  0.238 & 0.231 & 0.229 \\\\\n",
      "sg\n",
      "& 50  &  0.153 & 0.119 & 0.113 \\\\\n",
      "& 100  &  0.174 & 0.167 & 0.163 \\\\\n",
      "& 500  &  0.215 & 0.213 & 0.209 \\\\\n",
      "glove\n",
      "& 50  &  0.18 & 0.178 & 0.175 \\\\\n",
      "& 100  &  0.184 & 0.195 & 0.186 \\\\\n",
      "& 500  &  0.242 & 0.245 & 0.24 \\\\\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for method, scores in scores_d['kv'].items():\n",
    "    print(method)\n",
    "    for vector_size, windows in scores.items():\n",
    "#         print(windows.keys())\n",
    "        ws = [str(round(a,3)) for a in list(windows.values())]\n",
    "        print(\"&\",vector_size,\" & \",\" & \".join(ws),\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f27f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfa7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "def pos(language, vector_f, epochs=10, vector_load=True):\n",
    "    \n",
    "    NUM = data.Field(lower = True)\n",
    "    TEXT = data.Field(lower = True)\n",
    "    LEMMA = data.Field(lower = True)\n",
    "    UD_TAGS = data.Field(unk_token=None)\n",
    "        \n",
    "    fields = ((\"num\", NUM), (\"text\", TEXT), (\"lemma\", LEMMA), (\"udtags\", UD_TAGS))\n",
    "        \n",
    "    train_data, valid_data, test_data = datasets.UDPOS.splits(\n",
    "                                                            fields,\n",
    "                                                            root='./extrinsic/'+language,\n",
    "                                                            train='train.conll', \n",
    "                                                            test='test.conll',\n",
    "                                                            validation='valid.conll'\n",
    "                                                          )\n",
    "        \n",
    "    print(f\"Number of training examples: {len(train_data)}\")\n",
    "    print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "    print(f\"Number of testing examples: {len(test_data)}\")\n",
    "    \n",
    "    MIN_FREQ = 1\n",
    "    \n",
    "    if vector_load:\n",
    "        !rm -rf /tmp/vec    \n",
    "        _vectors = Vectors(name=vector_f, cache='/tmp/vec')\n",
    "        TEXT.build_vocab(train_data, \n",
    "                         min_freq = MIN_FREQ, \n",
    "                         vectors = _vectors,\n",
    "                         unk_init = torch.Tensor.normal_\n",
    "                        )\n",
    "        EMBEDDING_DIM = TEXT.vocab.vectors[1].shape[0]\n",
    "    else:\n",
    "        TEXT.build_vocab(train_data, \n",
    "                     min_freq = MIN_FREQ)\n",
    "        EMBEDDING_DIM = 100\n",
    "\n",
    "    UD_TAGS.build_vocab(test_data)\n",
    "    LEMMA.build_vocab(test_data)\n",
    "    NUM.build_vocab(test_data)\n",
    "    \n",
    "    print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "    print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "    \n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = device)\n",
    "    \n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.25\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    model = BiLSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT,\n",
    "                        PAD_IDX)\n",
    "    \n",
    "    model.apply(init_weights)\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "    \n",
    "    if vector_load:\n",
    "        pretrained_embeddings = TEXT.vocab.vectors\n",
    "        print(pretrained_embeddings.shape)\n",
    "    \n",
    "        model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    N_EPOCHS = epochs\n",
    "\n",
    "#     best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "    model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436067dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
