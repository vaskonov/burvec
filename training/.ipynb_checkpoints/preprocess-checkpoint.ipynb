{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "wiki = WikiCorpus('data/bxr/bxrwiki-latest-pages-articles.xml.bz2', lemmatize=False)\n",
    "\n",
    "total = 0\n",
    "with open('text.txt', 'w') as output:\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(\" \".join(text) + \"\\n\")\n",
    "        total += 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_bur_srt(word):\n",
    "\n",
    "    lets = [0x4ae, 0x4af, 0x4ba, 0x4bb, 0x4e8, 0x4e9, 0x401, 0x451, 0x68, 0x48]\n",
    "#     cyr_not = []\n",
    "    \n",
    "    for let in word:        \n",
    "        code = int(hex(ord(let)),16)\n",
    "        \n",
    "        if (code<0x410 or code>0x42f) and (code<0x430 or code>0x44f) and code not in lets :\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "print(check_bur_srt('people'))\n",
    "print(check_bur_srt('cinema capital'))\n",
    "print(check_bur_srt('rgb'))\n",
    "print(check_bur_srt('mall'))\n",
    "print(check_bur_srt('сырдајя'))\n",
    "\n",
    "print(check_bur_srt('харагшадые'))\n",
    "print(check_bur_srt('һүүлэй'))\n",
    "print(check_bur_srt('гоё'))\n",
    "print(check_bur_srt('гэхэ'))\n",
    "print(check_bur_srt('восток'))\n",
    "print(check_bur_srt('гэhэн'))\n",
    "print(check_bur_srt('һүүлэй'))\n",
    "print(check_bur_srt('мүнөө'))\n",
    "print(check_bur_srt('пионер'))\n",
    "print(check_bur_srt('нэрлэбэл'))\n",
    "print(check_bur_srt('байранууд'))\n",
    "print(check_bur_srt('юм'))\n",
    "\n",
    "print(check_bur_srt('хонин'))\n",
    "print(check_bur_srt('шүүһэн'))\n",
    "print(check_bur_srt('хубсаһан'))\n",
    "print(check_bur_srt('волга'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from 3 schar\n",
    "# sort by len\n",
    "\n",
    "suff= {\n",
    "    'plural_con':['ууд', 'үүд'],\n",
    "    'plural_vow': ['нууд', 'нүүд'],\n",
    "    'plural_n': ['гууд', 'гүүд'],\n",
    "    'plural_n1': ['гша', 'гшэ', 'гшо', 'ааша', 'ээшэ', 'оошо', 'өөшэ', 'д'],\n",
    "    'plural_pep': ['нар', 'нэр', 'нор'],\n",
    "    'lich_prityj': ['мни', 'ни', 'мнай', 'най', 'шни', 'тнай', 'иинь', 'ынь', 'нь'],\n",
    "    'bezlich_prityj': ['нгаа', 'нгээ', 'нгоо', 'нгөө', 'гаа', 'гээ', 'гоо', 'гөө', 'аа', 'ээ', 'оо', 'өө', 'гаа', \n",
    "                      'гээ', 'гоо', 'гөө', 'аа', 'ээ', 'оо', 'өө', 'яа', 'еэ', 'ёо', 'аа', 'ээ', 'оо', 'өө', \n",
    "                      'яа', 'еэ', 'ёо' 'гаа', 'гээ', 'гоо', 'гөө', 'н'],\n",
    "    'cases':['ай', 'эй', 'ой', 'гай', 'гой', 'ын', 'иин', 'н', 'да', 'до', 'дэ', 'та', 'тэ', 'то', 'ые', 'ы', \n",
    "             'иие', 'ии', 'е', 'гые', 'гы', 'аар', 'ээр', 'оор', 'өөр', 'яар', 'еэр', 'ёор', 'гаар', 'гээр', \n",
    "             'гоор', 'гөөр', 'тай', 'тэй', 'той', 'һаа', 'һээ', 'һоо', 'һөө', 'гһаа', 'гһээ', 'гһоо', 'гһөө'],\n",
    "    'verbs_present': ['б', 'м', 'бди', 'мди', 'ш', 'т'],\n",
    "    'past': ['ба', 'бэ', 'бо'],\n",
    "    'future': ['ха', 'хэ', 'хо'],\n",
    "    'negation': ['үдүй', 'дуй', 'үгы', 'гүй']\n",
    "}\n",
    "\n",
    "suffs= set([suf for l1 in suff.values() for suf in l1])\n",
    "\n",
    "def rem_suf(word):\n",
    "    if len(word)>4:\n",
    "        if word[-4:] in suffs:\n",
    "            return word[:-4]\n",
    "        if word[-3:] in suffs:\n",
    "            return word[:-3]\n",
    "    \n",
    "    return word\n",
    "\n",
    "print(rem_suf('тойрогһоо'))\n",
    "print(rem_suf('ябадалтнай'))\n",
    "print(rem_suf('сахижа'))\n",
    "print(rem_suf('хэ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter short words, non-byr chatacters\n",
    "\n",
    "def post_sen(sen):\n",
    "    mod_sen = []\n",
    "    for word in sen.split(\" \"):\n",
    "        word = word.rstrip()\n",
    "        \n",
    "        if len(word)<3:\n",
    "            mod_sen.append('UNK')\n",
    "            continue\n",
    "        \n",
    "        if check_bur_srt(word):\n",
    "#             mod_sen.append(word)\n",
    "            mod_sen.append(rem_suf(word))\n",
    "        else:\n",
    "            mod_sen.append('UNK')\n",
    "            \n",
    "            \n",
    "    return ' '.join(mod_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = []\n",
    "\n",
    "with open(\"./text.txt\") as file:\n",
    "    for l in file:\n",
    "        new_text.append(post_sen(l))\n",
    "        \n",
    "print('number of articles:', len(new_text))\n",
    "with open('./bur_text_no_suf.txt', 'w') as f:\n",
    "    for text in new_text:\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build clusters\n",
    "import sys\n",
    "vocab = []\n",
    "\n",
    "with open(\"./bur_text_no_suf.txt\") as file:\n",
    "# with open(\"./bur_text.txt\") as file:\n",
    "    for l in file:\n",
    "        vocab = vocab + l.rstrip().split(\" \")\n",
    "        \n",
    "print('the number of words:', len(vocab))\n",
    "\n",
    "uniq = set(vocab)\n",
    "print('the number of unique words:', len(uniq))\n",
    "uniq.remove('UNK')\n",
    "print('the number of unique words without UNK:', len(uniq))\n",
    "\n",
    "vocab = sorted(list(uniq))\n",
    "\n",
    "vocab_by_letter = {}\n",
    "for word in vocab:\n",
    "    if word[0] not in vocab_by_letter:\n",
    "        vocab_by_letter[word[0]] = []\n",
    "\n",
    "    vocab_by_letter[word[0]].append(word)\n",
    "\n",
    "idx = 0\n",
    "for let, words in vocab_by_letter.items():\n",
    "    print('letters index', idx, len(words))\n",
    "    if len(words)>100:\n",
    "        dists = [d3(word1, word2) for idx1, word1 in enumerate(words) for word2 in words[idx1+1:]]\n",
    "\n",
    "        with open('./let_no_suf/vocab.'+str(idx), 'w') as f:\n",
    "            for word in words:\n",
    "                f.write(word+'\\n')\n",
    "\n",
    "        with open('./let_no_suf/dists.'+str(idx), 'w') as f:\n",
    "            for dist in dists:\n",
    "                f.write(str(dist)+'\\n')\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "def padd(a, b):\n",
    "    maxlen=max(len(a), len(b))\n",
    "    return a.ljust(maxlen), b.ljust(maxlen)\n",
    "\n",
    "def d3(a, b):\n",
    "    if (a==b):\n",
    "        return 0\n",
    "    \n",
    "    a, b = padd(a, b)\n",
    "    n = len(a)-1\n",
    "    m=0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if a[i]!=b[i]:\n",
    "            m = i\n",
    "            break\n",
    "\n",
    "    if m==0:\n",
    "        return sys.float_info.max \n",
    "            \n",
    "#     print('n', n)\n",
    "#     print('m', m)\n",
    "    \n",
    "    summ = 0\n",
    "    for i in range(m,n+1):\n",
    "        summ += 1/(2**(i-m))\n",
    "    \n",
    "    return float((n-m+1)/m)*summ\n",
    "\n",
    "print(d3('astronomer', 'astronomically'))\n",
    "print(d3('astronomer', 'astonish'))\n",
    "print(d3(u'хэлэнэй', u'хэлэниинь'))\n",
    "print(d3(u'хэлэнүүдэй', u'хэлэниинь'))\n",
    "print(d3(u'хэлэнэй', u'шэнжэлэл'))\n",
    "print(d3('хэлэнэй', 'шэнжэлэл'))\n",
    "print(d3('a', 'b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = fcluster(Z, t=1, criterion='distance')\n",
    "print(max(cl))\n",
    "    \n",
    "clust = {}\n",
    "for idx, num in enumerate(cl):\n",
    "    if num not in clust:\n",
    "        clust[num] = []\n",
    "    \n",
    "    clust[num].append(vocab[idx])\n",
    "    if len(clust[num])>1:\n",
    "        print(clust[num])\n",
    " \n",
    "print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "th = []\n",
    "res = []\n",
    "\n",
    "for j in np.arange(0.0, 10.0, 0.25):\n",
    "    th.append(j)\n",
    "    cl = fcluster(Z, t=j, criterion='distance')\n",
    "    res.append(max(cl))\n",
    "    print(j)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(th, res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import *\n",
    "import fastcluster\n",
    "\n",
    "def build_clusters(linkage, th, folder):\n",
    "# common buffer for all more than 1 clusters\n",
    "# key: letter index\n",
    "# value: list of lists of clusters with words\n",
    "    buffer = {}\n",
    "    \n",
    "    print(\"load distances from:\", folder)\n",
    "\n",
    "    for i in range(0,31):\n",
    "        print('letter index:', i)\n",
    "        vocab = []\n",
    "        dists = []\n",
    "        buffer[i] = []\n",
    "    \n",
    "        with open('./'+folder+'/vocab.'+str(i), 'r') as f:\n",
    "            for l in f:\n",
    "                vocab.append(l.rstrip())\n",
    "\n",
    "        with open('./'+folder+'/dists.'+str(i), 'r') as f:\n",
    "            for l in f:\n",
    "                dists.append(float(l.rstrip()))\n",
    "            \n",
    "        \n",
    "    \n",
    "    #   Z = single(dists)\n",
    "    # cl = fcluster(Z, t=th, criterion='distance')\n",
    "    \n",
    "        Z = fastcluster.linkage(dists, method=linkage)\n",
    "        cl = fcluster(Z, t=th, criterion='distance')\n",
    "    \n",
    "        print(\"calculated\")\n",
    " \n",
    "        clust = {}\n",
    "        for idx, num in enumerate(cl):\n",
    "            if num not in clust:\n",
    "                clust[num] = []\n",
    "    \n",
    "            clust[num].append(vocab[idx])\n",
    "        \n",
    "        for key, value in clust.items():\n",
    "            if len(value)>1:\n",
    "                buffer[i].append(value)\n",
    "    \n",
    "        print('total clusters:', len(buffer[i]))\n",
    "        \n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(buffer[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_stem(word_lemma, filename):\n",
    "    post = []\n",
    "\n",
    "    print('load text from file:', filename)\n",
    "# load filtered text\n",
    "    with open(filename) as file:\n",
    "        for l in file:\n",
    "            post.append(l.rstrip().strip())\n",
    "\n",
    "    print('total articles:', len(post))\n",
    "    print(type(post[0]))\n",
    "    post_lemma = []\n",
    "\n",
    "    for idx, sen in enumerate(post):\n",
    "        if idx % 100==0:\n",
    "            print(idx)\n",
    "        mod = sen\n",
    "        for key, value in word_lemma.items():\n",
    "            mod = mod.replace(key, value)\n",
    "    \n",
    "        post_lemma.append(mod)\n",
    "    print('total articles lemmatized:', len(post_lemma))\n",
    "    \n",
    "#     with open('post_lemma.txt', 'w') as f:\n",
    "#         for sen in post_lemma:        \n",
    "#             f.write(sen+'\\n')\n",
    "\n",
    "    return post_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ONLY FOR NON_CLUSTER PERFORMANCE\n",
    "post_lemma = []\n",
    "\n",
    "with open(\"bur_text.txt\") as file:\n",
    "    for l in file:\n",
    "        post_lemma.append(l.rstrip().strip())\n",
    "\n",
    "for ng in [2,5,10]:\n",
    "    word_context = build_pairs(post_lemma, ng)\n",
    "    c = dict(Counter(word_context))\n",
    "    save(c, 'none', 0, post_lemma, {}, 'pmi', ng)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "# import sys\n",
    "# from collections import Counter\n",
    "\n",
    "def build_pairs(post_lemma, ng):\n",
    "    omit = ['BEG', 'END', 'UNK']\n",
    "    word_context = []\n",
    "    words = []\n",
    "\n",
    "    print('total articles', len(post_lemma))\n",
    "\n",
    "    for idx, text in enumerate(post_lemma):\n",
    "        \n",
    "        text = text.split(\" \")\n",
    "        \n",
    "        for i in range(ng):\n",
    "            text = ['BEG'] + text\n",
    "            text = text + ['END']\n",
    "\n",
    "        for idx in range(ng,len(text)-ng):\n",
    "            for i in range(idx-ng, idx):\n",
    "                if text[i] not in omit:\n",
    "                    word_context.append((text[idx], text[i]))\n",
    "                \n",
    "            for i in range(idx+1, idx+ng+1):\n",
    "                if text[i] not in omit:\n",
    "                    word_context.append((text[idx], text[i]))\n",
    "\n",
    "    print('build_pairs, ng:', ng, ' num of pairs:', len(word_context))\n",
    "    return word_context\n",
    "\n",
    "print(build_pairs(['I like apples and alcohol'],2))\n",
    "print(build_pairs(['I like apples and alcohol is this OK'],3))\n",
    "print(build_pairs(['I like apples and alcohol'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "print(list(ngrams(['BEG','BEG','I', 'like', 'apples','END','END'],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # key: words\n",
    "# # value: occurences\n",
    "# c_w = dict(Counter(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # pairs (word, occurences)\n",
    "# tups = [(k,v) for k,v in c_w.items()]\n",
    "# print('vocab total:', len(tups))\n",
    "\n",
    "# #  sort by occurences\n",
    "# # print(sorted(tups, key = lambda x: x[1]))\n",
    "\n",
    "# # only tuples that > 4\n",
    "# tups5 = set([w[0] for w in tups if w[1]>4])\n",
    "# print('vocab5 number:', len(tups5))\n",
    "\n",
    "# word_context5 = []\n",
    "# for tup in word_context:\n",
    "#     if tup[0] in tups5:\n",
    "#         word_context5.append(tup)\n",
    "        \n",
    "# print('word_context:', len(word_context))\n",
    "# print('word_context5:', len(word_context5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # c = dict(Counter(word_context5))\n",
    "# c = dict(Counter(word_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save(c, linkage, th, post_lemma, word_lemma, folder, ng):\n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []\n",
    "\n",
    "    for key, value in c.items():\n",
    "        rows.append(key[0])\n",
    "        cols.append(key[1])\n",
    "        data.append(key[0] + \" \" + key[1] + \" \" + str(value))\n",
    "    \n",
    "    rows = np.unique(rows)\n",
    "    cols = np.unique(cols)\n",
    "\n",
    "    with open('./'+folder+'/'+linkage+'-'+str(th)+'-'+str(ng)+'-cols', 'w') as f:\n",
    "        for col in cols:\n",
    "            f.write(col + \"\\n\")\n",
    "\n",
    "    with open('./'+folder+'/'+linkage+'-'+str(th)+'-'+str(ng)+'-rows', 'w') as f:\n",
    "        for row in rows:\n",
    "            f.write(row + \"\\n\")\n",
    "\n",
    "    with open('./'+folder+'/'+linkage+'-'+str(th)+'-'+str(ng)+'-data', 'w') as f:\n",
    "        for dt in data:\n",
    "            f.write(dt + \"\\n\")\n",
    "            \n",
    "    with open('./'+folder+'/'+linkage+'-'+str(th)+'-'+str(ng)+'-lemma', 'w') as f:\n",
    "        for key, value in word_lemma.items():        \n",
    "            f.write(key+'\\n')\n",
    "            f.write(value+'\\n')\n",
    "    \n",
    "    with open('./'+folder+'/'+linkage+'-'+str(th)+'-'+str(ng)+'-text', 'w') as f:\n",
    "        for txt in post_lemma:        \n",
    "            f.write(txt+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# FloatingPointError: NaN dissimilarity value in intermediate results.\n",
    "# centroid\n",
    "# median\n",
    "# ward\n",
    "\n",
    "\n",
    "# method='mcquitty':\n",
    "\n",
    "linkages = ['average', 'complete']\n",
    "\n",
    "for linkage in linkages:\n",
    "    \n",
    "    for th in np.arange(4,6,0.1):\n",
    "        print('LINKAGE:', linkage, 'th:', th)\n",
    "        \n",
    "        buffer = build_clusters(linkage, th, 'let')#let_no_suf\n",
    "    \n",
    "        buffer_global = []\n",
    "        for key, value in buffer.items():\n",
    "            buffer_global += value\n",
    "\n",
    "        print('total clusters:',len(buffer_global))\n",
    "        \n",
    "        if len(buffer_global)>100:\n",
    "            word_lemma = {}\n",
    "\n",
    "            for cluster in buffer_global:\n",
    "                lemma = min(cluster, key=len)\n",
    "                for word in cluster:\n",
    "                    if word != lemma:\n",
    "                        word_lemma[word] = lemma\n",
    "\n",
    "            post_lemma =  replace_stem(word_lemma, 'bur_text.txt')#bur_text_no_suf.txt\n",
    "            \n",
    "            for ng in [2,5,10]:\n",
    "                word_context = build_pairs(post_lemma, ng)\n",
    "                c = dict(Counter(word_context))\n",
    "                save(c, linkage, th, post_lemma, word_lemma, 'pmi',ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO LEMMATIZATION\n",
    "post_lemma =  replace_stem({})\n",
    "word_context = build_pairs(post_lemma)\n",
    "c = dict(Counter(word_context))\n",
    "save(c, 'none', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_lemma = []\n",
    "\n",
    "with open(\"./pmi/average-1.5-text\") as file:\n",
    "    for l in file:\n",
    "        post_lemma.append(l.rstrip().strip())\n",
    "\n",
    "for ng in [2,5,10]:\n",
    "    word_context = build_pairs(post_lemma, ng)\n",
    "    c = dict(Counter(word_context))\n",
    "    save(c, 'average', '1.5', post_lemma, {}, 'pmi' ,ng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
